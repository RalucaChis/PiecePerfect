{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSe6hfLsMIh1"
   },
   "source": [
    "### Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9hbKTjyEoVZ",
    "outputId": "f76098c3-5098-4fac-9589-45b2b8214eca",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O6DVk0qNYLn"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jle6924e5hlv"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2-G3VYL5j5K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "sdZQVM-WE0QO",
    "outputId": "f5e0439a-4010-4dac-c7f1-1f3f6ee943a9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "import datetime\n",
    "import pickle\n",
    "import errno\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import PIL.ImageOps  \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import wandb\n",
    "wandb.login()\n",
    "# c390abcd434dcb1654d8e2597e93b0f775494b47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhrP5H3HMqd3"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H778Q3aod2o9"
   },
   "outputs": [],
   "source": [
    "class PuzzleDataset(object):\n",
    "    def __init__(self, imgs1, imgs2, match_types, pair_types):\n",
    "        self.root = '/content/gdrive/MyDrive/puzzle_pieces'\n",
    "        self.imgs1 = imgs1\n",
    "        self.imgs2 = imgs2\n",
    "        if not torch.is_tensor(imgs1) and not torch.is_tensor(match_types):\n",
    "          self.match_types = torch.from_numpy(match_types)\n",
    "        if not torch.is_tensor(imgs1) and not torch.is_tensor(pair_types):\n",
    "          self.pair_types = torch.from_numpy(pair_types)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.imgs1)\n",
    "\n",
    "    def get_image(self, path):\n",
    "      with open(os.path.abspath(path), 'rb') as f:\n",
    "          with Image.open(f) as img:\n",
    "              return img.convert('L') \n",
    "\n",
    "    def get_rotated_90(self, path):\n",
    "      with open(os.path.abspath(path), 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "          return img.convert('L').rotate(-90)\n",
    "\n",
    "    def get_rotated_180(self, path):\n",
    "      with open(os.path.abspath(path), 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "          return img.convert('L').rotate(-180)\n",
    "\n",
    "    def get_rotated_270(self, path):\n",
    "      with open(os.path.abspath(path), 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "          return img.convert('L').rotate(-270)      \n",
    "\n",
    "    # def get_image_inv(self, path):\n",
    "    #   with open(os.path.abspath(path), 'rb') as f:\n",
    "    #       with Image.open(f) as img:\n",
    "    #           return PIL.ImageOps.invert(img.convert('L'))\n",
    "\n",
    "    # def get_rotated_90_inv(self, path):\n",
    "    #   with open(os.path.abspath(path), 'rb') as f:\n",
    "    #     with Image.open(f) as img:\n",
    "    #       return PIL.ImageOps.invert(img.convert('L').rotate(-90))\n",
    "\n",
    "    # def get_rotated_180_inv(self, path):\n",
    "    #   with open(os.path.abspath(path), 'rb') as f:\n",
    "    #     with Image.open(f) as img:\n",
    "    #       return PIL.ImageOps.invert(img.convert('L').rotate(-180))\n",
    "\n",
    "    # def get_rotated_270_inv(self, path):\n",
    "    #   with open(os.path.abspath(path), 'rb') as f:\n",
    "    #     with Image.open(f) as img:\n",
    "    #       return PIL.ImageOps.invert(img.convert('L').rotate(-270))      \n",
    "             \n",
    "    def __getitem__(self, idx):\n",
    "      path_to_images = os.path.join(self.root, f'masks_crop_tight')\n",
    "      path_to_img1 = os.path.join(path_to_images, self.imgs1[idx])\n",
    "      path_to_img2 = os.path.join(path_to_images, self.imgs2[idx])\n",
    "\n",
    "      if self.pair_types[idx] == 1:\n",
    "        img1 = self.get_rotated_90(path_to_img1)\n",
    "        img2 = self.get_rotated_90(path_to_img2)\n",
    "      elif self.pair_types[idx] == 3:\n",
    "        img1 = self.get_rotated_270(path_to_img1)\n",
    "        img2 = self.get_rotated_270(path_to_img2)\n",
    "      elif self.pair_types[idx] == 4:\n",
    "        img1 = self.get_rotated_180(path_to_img1)\n",
    "        img2 = self.get_rotated_180(path_to_img2)\n",
    "      elif self.pair_types[idx] == 2:\n",
    "        img1 = self.get_image(path_to_img1)\n",
    "        img2 = self.get_image(path_to_img2)\n",
    "      else:\n",
    "        img1 = self.get_image(path_to_img1)\n",
    "        img2 = self.get_image(path_to_img2)\n",
    "      \n",
    "      img1 = self.transform(img1)\n",
    "      _,w,h = img1.size()\n",
    "      img1 = transforms.functional.crop(img1,0,int(w/2)+10,h,int(w/2)-10) # top, left, height, width\n",
    "\n",
    "      img2 = self.transform(img2)\n",
    "      _,w,h = img2.size()\n",
    "      img2 = transforms.functional.crop(img2,0,0,h,int(w/2)-10)\n",
    "      # print(path_to_img1, path_to_img2, self.pair_types[idx])\n",
    "      return img1, img2, self.match_types[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9xRt2_EMpzQ"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/gdrive/MyDrive/puzzle_pieces/puzzle_pairs_train.csv')\n",
    "val_df = pd.read_csv('/content/gdrive/MyDrive/puzzle_pieces/puzzle_pairs_val.csv')\n",
    "test_df = pd.read_csv('/content/gdrive/MyDrive/puzzle_pieces/puzzle_pairs_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZO7lI6aFtDW"
   },
   "outputs": [],
   "source": [
    "def get_img(data_df, col_img1, col_img2, col_match_type, col_pair_type):\n",
    "  img1 = data_df[col_img1].astype('string').to_numpy()\n",
    "  img2 = data_df[col_img2].astype('string').to_numpy()\n",
    "  match_type = data_df[col_match_type].astype('float').to_numpy()\n",
    "  pair_type = data_df[col_pair_type].astype('float').to_numpy()\n",
    "  return img1, img2, match_type, pair_type\n",
    "\n",
    "img1_train, img2_train, match_type_train, pair_type_train = get_img(train_df, 'img1_name', 'img2_name', 'match_type', 'pair_type')\n",
    "img1_test, img2_test, match_type_test, pair_type_test = get_img(test_df, 'img1_name', 'img2_name', 'match_type', 'pair_type')\n",
    "img1_val, img2_val, match_type_val, pair_type_val = get_img(val_df, 'img1_name', 'img2_name', 'match_type', 'pair_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxt2rSC7Fwkz"
   },
   "outputs": [],
   "source": [
    "dataset_train = PuzzleDataset(img1_train, img2_train, match_type_train, pair_type_train)\n",
    "dataset_test = PuzzleDataset(img1_test, img2_test, match_type_test, pair_type_test)\n",
    "dataset_val = PuzzleDataset(img1_val, img2_val, match_type_val, pair_type_val)\n",
    "\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(dataset_train, batch_size=64, shuffle=True, num_workers=2),\n",
    "               'val': torch.utils.data.DataLoader(dataset_val, batch_size=64, shuffle=True, num_workers=2),\n",
    "               'test': torch.utils.data.DataLoader(dataset_test, batch_size=64, shuffle=True, num_workers=2)\n",
    "              }\n",
    "dataset_sizes = {'train': len(dataset_train), 'val': len(dataset_val), 'test': len(dataset_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSBGFZtuFEzk",
    "outputId": "06cc5d64-e7d2-431d-d4e8-72ecc8737b35"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train': 2322, 'val': 664, 'test': 332}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDLXfMXLjGfm",
    "outputId": "55257904-8dd9-42e6-a960-d0cc55511e24"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 ariel_60.png ariel_49.png 3 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9E1402F50>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAAxUlEQVR4nO2WwQ3CMAwAE8Qa7MNISHzZiZEYw3wQTWKQY1NUS717tE1Un6zUcVMKAAAArEqVdhAQHH7NAMEgOEUM0nAPxHd1ECmEZIuIAAGCtQSPzTMI0Le0padJmWxwXzKQ9yUmkO7mF4h6mBbchrAJg/RchxkzvlqvWJ9i+0JCYAvMDfXvDOwdffwYIu0gkkGdjld7wX3Myl8HCAKCs1egmqq3kvKtAYIUgotToH/vzlJMuAYIdil4Fa6oGQAAAACAnfMEoJI93psFZBMAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9057C3280>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABMklEQVR4nO2XuxHCMAyGJY6CkjZHQ8kIsAGMwghswiaMwA4sABtQQkEejp+yDbk4/F+VxNFvWZZ0NhEAAAAABuRlhejS/cJ+gfAcs0wfIfAPAsewgD8TBak49hhAYBoCu997EKiFcDGMPogQ+LLAmpk5lDcGC+VU8GGvHxYCmH9uewJBlxpDNj8RkTwG7HgOC7Bp03uTeaAv9OEeEtKFoahMVOlWXuwSyt+Fe/eYlolKPSZ5oNbzXGigelpFT9jvSJu4jkR6U9K6oEDA3zdLTSQIxArcvKMxmZjqgZdKVI1OOM+DFZOwHzinpxwP6vAnCzTbJxE4WL6ddCEvlkRo7YqohakKtBszjAe2ZFl6xkxsLSW3FhogMFBD8dmn3VjO8dfH9kx1jbWseSbaAQAAACCaN8p0eJd3b8WqAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "imagine=dataset_test.__getitem__(6)\n",
    "print(int(imagine[2]), test_df.loc[6][0], test_df.loc[6][1], test_df.loc[6][2], test_df.loc[6][3])\n",
    "pil_image = torchvision.transforms.ToPILImage()(imagine[0])\n",
    "pil_image.show()\n",
    "pil_image = torchvision.transforms.ToPILImage()(imagine[1])\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEYohXkQjSbp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "37f077a4-2070-45ac-c619-267b05803893"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 cinderella_44.png cinderella_33.png 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9E21271F0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAArElEQVR4nO3WsQ3CMBBG4f8QBWzANKzIGEzADF6J7igARZEdcocLDLyvi6K82JFtRQIAAACAH3b0rsdt9vj+2hmQJMsFNulXEiBA4H8Cu1ygPpGSR9J28Y7HUo1vcJDk7vfGqsYUZrffGUEOgfEDl/XA64UUWMv1XjDF1vBDewpm0QHUAZsSoX29vJ2Dx8Lo6+ArA8m/xBGn8PnAOV0oRaU8L0694wEAAACAyQ2ALhLJJyubHAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9E2127250>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAAoklEQVR4nO3VMQ6DMAyFYZupt+mxelWOwtC94+uA2tCGxXYhqvR/U4TIw4mcYAYAAAAAB/HQ2+onTNEvSpIuhQo2vBpgZoklEEAAAftip/F1HL2NExVcff2qJ+6DnYIGb+JSXYJp9BKGb+LNipuoRAXS42N+8iyYv8fJgGb8fUBAOODePYm2ctcIf7cHBPwioOub0yv46sS5+GcDAAAAgCM9AU6+Gn4nekp+AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 zana_41.png zana_56.png 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9057C3280>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABF0lEQVR4nO2YsRHCMAxFDRd6WjaAjRiBEdiAkRghFQswAA3cpaSgEA0xJpYdS7mAwv1X5Yz9LBHFjuMcAAAAAMD/sR44fkb+SieY+ytquSkjCBtVESixKRDdD5spTF8geyhMpvBzgXBhsZjCUIF0bTWYwvQFJ6mguzOJd8ix/4NtryDaXN85UNRSFEHTDvdmeogES6pdTRQGVjXdTgHs+0HcSxABS2aWwtu4SP5SlkImCXsPEwRJ0qXICs7dhn1aXSVmu6yK5ucq0fcuOwhkFpSXomeVZFLgZSkmUwffFNyFgkwdKCOQYlFwlQmYZR3nhZK3khBub8TR17mDRIBvKOMIdqJK/NzaNN/TiDZHxTAAAAAAAMM8Ad3ZL58lG/d+AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9E2127100>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABI0lEQVR4nO2XsQ3CQAxFbQQFCNHTIlEwQjagYwxGQIxAxwQsRMUGdBRsAIVpEkIOI50xJBz5r7p8xV/Ov8vlQgQAAAAAAMAfICLLqsJGg6eyNw3KWo8BERF1nPVGA4XEDA6KZpoFJcPEMtCewGTQ10RLiO4OVOo0mKuqIQM1grQy8BpMdDk+RD1DQwfrcjjmcXTZYws5+eWuchXFKrjfbBBy8hrQSUSMX6YASWslarDXIP0MFmTe1jkU7HsiB0L8I+SrXqRXEd7I4CIbOd5foxp3pBfUaHDW5fgMulenAb6NnzAYqmq7Tmn/YDDSxJb9sRBvi1E2YGbOyPzvHOI7IhERSWIhfsFAmu5g710H6Z/SpPEOptT4LMxc1UUPAAAAwC9xAwDAT2Js9/qhAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 frozen_50.png frozen_51.png 4\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF90577F460>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABFklEQVR4nO2XMQ7CMAxFE4SEuA8D12DjBFyFIzGzIbFwBSaOYYZGVUns4NRtlVT/TVVUv9hum6TOAQAAAAAAUCUfq4A4XlbBWR/viR9WCzb6uUoEKygBAggggKDnulwGwpqoXxQXauKbiIjPdauJD6HE1SVlcEjjBxcDpCYOukjs6L8M8tNqBI6eTHxqE0sQSGoofQ+S+ar9GmVO8QB7yMpwj+JLn0LyHBpsolkQv4rtlZB8TLklrbt59HrgPTNnwYqUC1IJPHfJuFRNDNVcuFykJu7ifNhNwbWzN0Iws0D/hzhXBnaBsYYaSjALinfXqTMwJlBFEyGAYB0C7/dWR8fx91R/08b1J5SH6lCXEQRGagAAYAK+f9WNU5at7q8AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF90577F6A0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABYUlEQVR4nO2YMW5CMQyGbYRQJ2YWFqaKo/QEXKE36NCdK3CTDr0AR+g1mGjFEAaQ0IsdY+Ogl1T+FwQ4n+3E8csLQCgUqqE0cY4HJwACUEFp7ABCoSb0mRIW/lJvktJ23ngBapUApdTqR1D0pJ3F8ZtqAALw3wHahtBwCq0B1oiI7LyWJ/s0pVZMoxRWKzFGlCCksGecqB8XF3cXLZjfbhIJS8YiB9hiAjoL5jrIPXZTiTUBH8Ov5lXI12GEFLJC6HEVvADbXni7v2PFOjjMiRGhiYBErGZ/uY2Uws1b2l4/yXhRg86zAfgm/exOR1KddjsspAAEIAAB6BOwGz0ClXTPxmdGEIBnAxAREX9EgOWw/XJ0AgCY2rLOAb47I6hwWs9cNl4HjQJWTo8L59u7txJNVyA62QBMmzYBuDbfYyF5AFzR2CJgCMYUfjVMWf6LKP8lzCCIR8ZfKV+vD48NhYjOOxqNEjehHt8AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 rapunzel_28.png rapunzel_20.png 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9057C3550>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAA0klEQVR4nO2WwRGCMBBF/+ckJXi0DK2AVqxA7cBuLE9v60nRSSZmXQYz+N+JIeTx2WwAQAghhBBCTA7teXjCOSZ4PVsv6L646V8Itj9P4CHbB442aOARJGhV0HsEuU70NGKjNXhwMDOzXVmQq0F/A4BxoFiTXILr+3ysvAnSi5wJEjbRBKUIbffBdILSh2aWZSxu75oE7lYGSXJdNR+wlHFwMBs+xVv2G0kCCSSQYG4BSZL7qEhEOEYFVX9pBTpcogmCEVrYC1HiqyCEEEIIIRbIHb/lMPBk6GerAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9E2126980>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAApUlEQVR4nO3WQQqDMBSE4bF00d5/0St4I3uDHmNcmICCkWcChsr/bYLiG5+JESUAAAAAuK2httCpui1A0qM2ICOAgL4B7zRW74W8Gf54Dm4U8DxzsXeWPd7Bz5Zk2zlM0okXyYXz0Q5K9dGAYv1Vy/hpDTgQXIXuc3DQaLSDYkL4EbYJY7RsbfJiOfLmqMIrBfBV7hnwTSP/iQS0mfrdGgAAAMAlZkHQK/VwoAwUAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 frozen_35.png frozen_26.png 3\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9057D0760>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABgUlEQVR4nO2YsU7EMAyGf0cVYmC55SQmNiRWNt6ImZfgPXgLViY22HkBJCQ2JJjMUK60Seo6Ncclkr+tafyf4zj2pYDjOI7jOE5tEACAxw9lBADPP/bggbMiD1h4p/Fgzl5HMFnLAvdWgSergI6wbvenHrxYBc51q50XwKVVwBKGfSZSKwI915yhRCBnz7cq0wDYKkJnLSidzbyGbaT5JWzfjB7cqDyoOgYfJ0YBdWtTccTMzJvVAndfAID3jL/Zg5ic52HsokTgITcptpeCOISR06EBOQbc/3wyNKITBRS1ojwTI83Dn4UmBY4nT3IiZZmmUpMxOLhAdBzNHiycRoCi41NWD0AEgOhKmKGpSPj1Iin1ogeviVTaKkSB07Ez9NgvKEJcgqY1tZiJ/yqwVQiQ2DsU2xCym1tAAGwS1dyZ1rtQeSLZBRR9L6hnigKGe4/51haM9n/xHWmxN8nI9UCR4vvORM1fXVtBQgA+TQLS98Td+yUP5P7vOI7jOE5rfAP/c7LZm97FkAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9E2127100>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABJElEQVR4nO2YMW4CMRBFZ6JIcIR0kRDiHjldjkJJmz4NN0A5ARegASH9FLC7rHbC2DuAY/RfgzS2H59ZYwtECCGEEJIFAGAaFbR8RwVnllFBw4+IugJn/CUjDAUUVCzYFk9wd4F6golrcMa988RN4PEWTVD+KVBQQLBWdS/DPr279NMoeoK5OfeimHE7q1l1e9AuOzrjXga1imkCEQyn5Qls6YnqdmLH4vw6ugdNEyruAQXPsJXbr/NDEgAADnaA9CNt9KHazYVRy/3RpcNaXhMxFxFYzsQEBvXuRAr+kWCvr6H1GkygUrgHGhWEE6QJvpxx/1C9cqIUfwqCcAKEPwLeQ00UKd1EClIFH9G3EJHl3//rrsb4Zptm+e4G6QghhBDS5xeXVcVLGVFiigAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 zana_62.png zana_38.png 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9057D39D0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABaklEQVR4nO2YMW7CQBBFZyyUJkWK3IAKKtrkAhEnSMQJkooaKZdImYNEok6ZE6SjhZo2xVCAwF7Wm1l/W9ri/46R5/nPetgdW4SiKIqiqIbspEnHfLXmTxSQD4kA8jjtACej8t2HgCQgv3X8Dt48AJX2RnCZqzq0fwAAVTTg0QVIPAXf6lby47puQAepDcUFAPP9gPHv7CUWT+yJtTWwSOx/B+es1eUmkfPHEvoSkXkQyyihRUEV+U/hHXUQWCj670zAcAC0E/8yHehdELjJAqjKXnVxCTxfbQjOIcuuIj4HNSuqH6qRjXrQMe/WkZ8E3HsAqRKcE0q7XlEHnJUJKATgOXgLL4GAHMC0+5vXSKTLd4uGg6XK8ehRHesnwKrrIT7XxpS6yL5BgEcl9AFYQx8lbCGAGtaIUolsMAtmM2wZzdY2AvKPAwawCiV0Yh+AJ4RgZjskX03gPRFuZfBj2hpLpyiKoiiqbx0AL7hvGvTNiWMAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9057D0850>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABLElEQVR4nO2YzU3DQBCF3+YQhT44pATSASVQAtAHSgEpgluu4c4hLdCJwwEeByQL2TubmThYa+t9J9uz+/R2dqz9AYQQQgghRF0cSKbLurJ9WkS7PpAkz7fLsGYO5xCalRXxCRQsu3LwWIiFk9hlnCGUGElgY4ecdWAnYSo5kMA1BG7MiLOQFl8DHSyHOrBreTKzgA8rUHEOtvc+gfwQ2I2m75CDdsAk3wGAZv+sg9Dam3EQW7v/YRaCm4fp/AsjCqyDAv1CmsEsBDe/Hge7t0Lw/N+YAGDVRBz8EX1Jvy+nkIPWROp9cQtYkn1qrIPZCsxgfyABnVhqEPi0Q2Mdvp+ffO0K3O6zVyihtfwO5PE4QAAAcALYIHyL04Xpsqukng5er6IjhBBCCFEXPz6EfFHvamvlAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 ariel_60.png ariel_49.png 3\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF9E2127190>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAAxUlEQVR4nO2WwQ3CMAwAE8Qa7MNISHzZiZEYw3wQTWKQY1NUS717tE1Un6zUcVMKAAAArEqVdhAQHH7NAMEgOEUM0nAPxHd1ECmEZIuIAAGCtQSPzTMI0Le0padJmWxwXzKQ9yUmkO7mF4h6mBbchrAJg/RchxkzvlqvWJ9i+0JCYAvMDfXvDOwdffwYIu0gkkGdjld7wX3Myl8HCAKCs1egmqq3kvKtAYIUgotToH/vzlJMuAYIdil4Fa6oGQAAAACAnfMEoJI93psFZBMAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF90577F6A0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABMklEQVR4nO2XuxHCMAyGJY6CkjZHQ8kIsAGMwghswiaMwA4sABtQQkEejp+yDbk4/F+VxNFvWZZ0NhEAAAAABuRlhejS/cJ+gfAcs0wfIfAPAsewgD8TBak49hhAYBoCu997EKiFcDGMPogQ+LLAmpk5lDcGC+VU8GGvHxYCmH9uewJBlxpDNj8RkTwG7HgOC7Bp03uTeaAv9OEeEtKFoahMVOlWXuwSyt+Fe/eYlolKPSZ5oNbzXGigelpFT9jvSJu4jkR6U9K6oEDA3zdLTSQIxArcvKMxmZjqgZdKVI1OOM+DFZOwHzinpxwP6vAnCzTbJxE4WL6ddCEvlkRo7YqohakKtBszjAe2ZFl6xkxsLSW3FhogMFBD8dmn3VjO8dfH9kx1jbWseSbaAQAAACCaN8p0eJd3b8WqAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "for i in range(7):  \n",
    "  img=dataset_test.__getitem__(i)\n",
    "  print(int(img[2]), test_df.loc[i][0], test_df.loc[i][1], test_df.loc[i][2])\n",
    "  pil_image = torchvision.transforms.ToPILImage()(img[0])\n",
    "  pil_image.show()\n",
    "  pil_image = torchvision.transforms.ToPILImage()(img[1])\n",
    "  pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr2W_Ehcdk7p"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SB_c6sVmdl64"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11,stride=1),\n",
    "            nn.Conv2d(64, 64, kernel_size=11,stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=5,stride=1,padding=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=5,stride=1,padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128,256, kernel_size=3,stride=1,padding=1),\n",
    "            nn.Conv2d(256,256, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(20480 , 1024),\n",
    "            nn.ReLU(inplace=True), \n",
    "            # nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(128,2),\n",
    "            nn.Softmax(1)\n",
    "        )    \n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.cnn1(input1)\n",
    "        output2 = self.cnn1(input2)\n",
    "        # print(output1.size())\n",
    "        \n",
    "        output1 = abs(output1-output2) \n",
    "        output1 = output1.view(output1.size()[0], -1)\n",
    "        output1 = self.fc1(output1)\n",
    "        return output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy6r_Q8lazrD"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwG3mHtEo7ym"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = -1\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for _, (inputs1, inputs2, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                labels = labels.type(torch.LongTensor).to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs1, inputs2)\n",
    "                    # labels = labels.reshape((labels.shape[0], 1))\n",
    "\n",
    "                    # log_probs = F.log_softmax(outputs, dim=1)\n",
    "                    # loss = F.nll_loss(log_probs, labels)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            if phase == 'train':\n",
    "              train_loss_history.append(float(epoch_loss))\n",
    "            else:\n",
    "              val_loss_history.append(float(epoch_loss))\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} ')\n",
    "            wandb.log({f'{phase} Epoch Loss': epoch_loss}, step=epoch) \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and (epoch_loss < best_loss or best_loss == -1):\n",
    "                print(f'Best loss was changed from {best_loss} to', end=' ')\n",
    "                best_loss = epoch_loss\n",
    "                print(f'{best_loss}')\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # torch.save(model, f\"/content/gdrive/MyDrive/models/test.pt\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_loss:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc_history, train_loss_history, val_acc_history, val_loss_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "WSGtSmKuXdNB",
    "outputId": "449c1cd7-e2c4-4009-dfdb-c6120101c663",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork().to(device)\n",
    "# print(model)\n",
    "\n",
    "config_defaults = {\n",
    "  'learning_rate': 0.0001, #1e-3\n",
    "  'epochs': 30,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "   config=config_defaults,\n",
    "   project=\"edge-match\", \n",
    "   entity=\"disertatie\",\n",
    "   name=f\"test-no-dropout\"\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=wandb.config.learning_rate, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=0.0005)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVUy0X53mwVW",
    "outputId": "41c6564c-94c2-4a89-af70-27c5163165f7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [02:33<00:00,  4.16s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.6295 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5419 \n",
      "Best loss was changed from -1 to 0.5418717107140875\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.5374 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5373 \n",
      "Best loss was changed from 0.5418717107140875 to 0.5373194609061781\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:28<00:00,  1.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.5160 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5136 \n",
      "Best loss was changed from 0.5373194609061781 to 0.5135763330631945\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.5025 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5132 \n",
      "Best loss was changed from 0.5135763330631945 to 0.5131590833146888\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.5061 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5648 \n",
      "\n",
      "Epoch 5/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4908 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5050 \n",
      "Best loss was changed from 0.5131590833146888 to 0.5049695860908692\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4818 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5451 \n",
      "\n",
      "Epoch 7/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:28<00:00,  1.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4612 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4970 \n",
      "Best loss was changed from 0.5049695860908692 to 0.4969839360340532\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:28<00:00,  1.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4514 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  2.93it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4993 \n",
      "\n",
      "Epoch 9/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4525 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4839 \n",
      "Best loss was changed from 0.4969839360340532 to 0.48391134875366487\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4476 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5008 \n",
      "\n",
      "Epoch 11/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4440 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  2.98it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4950 \n",
      "\n",
      "Epoch 12/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4417 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4953 \n",
      "\n",
      "Epoch 13/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4436 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.5224 \n",
      "\n",
      "Epoch 14/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4444 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4865 \n",
      "\n",
      "Epoch 15/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4358 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4878 \n",
      "\n",
      "Epoch 16/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4350 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  2.99it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4887 \n",
      "\n",
      "Epoch 17/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:28<00:00,  1.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4352 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4913 \n",
      "\n",
      "Epoch 18/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4344 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  2.75it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4924 \n",
      "\n",
      "Epoch 19/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4334 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4900 \n",
      "\n",
      "Epoch 20/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4332 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  2.76it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4898 \n",
      "\n",
      "Epoch 21/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4334 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.14it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4899 \n",
      "\n",
      "Epoch 22/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4331 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4921 \n",
      "\n",
      "Epoch 23/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4324 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4902 \n",
      "\n",
      "Epoch 24/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4341 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4905 \n",
      "\n",
      "Epoch 25/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:28<00:00,  1.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4323 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4909 \n",
      "\n",
      "Epoch 26/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4340 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4932 \n",
      "\n",
      "Epoch 27/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4322 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4910 \n",
      "\n",
      "Epoch 28/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:28<00:00,  1.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4322 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4923 \n",
      "\n",
      "Epoch 29/29\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 37/37 [00:27<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4333 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.49it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val Loss: 0.4925 \n",
      "\n",
      "Training complete in 17m 53s\n",
      "Best val Acc: 0.483911\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, train_acc, train_loss, val_acc, val_loss = train_model(model, criterion, optimizer, exp_lr_scheduler, wandb.config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8EySBwepMpy"
   },
   "outputs": [],
   "source": [
    "torch.save(model, f\"/content/gdrive/MyDrive/models/test-no-dropout.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZn57Q2nluDc"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Zu9pxbAFcTK"
   },
   "outputs": [],
   "source": [
    "model_path = \"/content/gdrive/MyDrive/models/test-no-dropout.pt\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjWAQ0JHFdnH"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "true_labels = []\n",
    "inputs = []\n",
    "with torch.no_grad():\n",
    "  for inputs1, inputs2, labels in dataloaders['test']:\n",
    "    inputs1 = inputs1.to(device)\n",
    "    inputs2 = inputs2.to(device)\n",
    "    for index in range(len(inputs1)):\n",
    "      inputs += [(inputs1[index], inputs2[index])]\n",
    "    labels = labels.to(device)\n",
    "    true_labels += labels\n",
    "    with torch.set_grad_enabled(False):\n",
    "      outputs = model(inputs1, inputs2)\n",
    "      preds += outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNQG1PefbL9b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "148a1583-3ce4-4583-d47c-c4c85ca1ac93",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHoV38TYFfRz"
   },
   "outputs": [],
   "source": [
    "acc = 0\n",
    "predicted_labels = []\n",
    "for index in range(len(dataset_test)):\n",
    "  label = int(true_labels[index])\n",
    "  true_labels[index] = int(true_labels[index])\n",
    "  predicted_labels.append(int(label) if preds[index][label] >= 0.5 else abs(int(label-1)))\n",
    "  if preds[index][label] >= 0.5:\n",
    "    acc += 1\n",
    "  # else:\n",
    "  #   print('actual: ', label,'pred: ', preds[index], index, '----------------')\n",
    "  #   print(test_df.loc[index][0], test_df.loc[index][1], test_df.loc[index][2])\n",
    "  #   pil_image = torchvision.transforms.ToPILImage()(inputs[index][0])\n",
    "  #   pil_image.show()\n",
    "  #   pil_image = torchvision.transforms.ToPILImage()(inputs[index][1])\n",
    "  #   pil_image.show()\n",
    " \n",
    "  # print(\"Predicted: \", preds[index])\n",
    "  print(\"Actual Label: \",label, \" Predicted: \", preds[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_xDfj8yAMJJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2bedd1ae-1ef5-4d0d-9281-d4267acec751"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "correct predicitons 281 out of 332\n"
     ]
    }
   ],
   "source": [
    "print('correct predicitons '+ str(acc) + ' out of ' + str(len(dataset_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLn8FH0M8EFG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "15a5da98-fbfe-4f7c-8e72-f744e0c5e09c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score: 0.8495575221238938\n"
     ]
    }
   ],
   "source": [
    "# compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17qnP-2vES9z"
   },
   "source": [
    "### Get top 10 matches for a piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uGqNUg1Rkic"
   },
   "outputs": [],
   "source": [
    "model_path = \"/content/gdrive/MyDrive/models/test-no-dropout.pt\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0pjxWLGGEDL"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "pairs = []\n",
    "top_5 = {}\n",
    "id=4\n",
    "with torch.no_grad():\n",
    "  img_compared = dataset_test.__getitem__(id)[0]\n",
    "  first_label = dataset_test.__getitem__(id)[2]\n",
    "  first_img = dataset_test.__getitem__(id)[0]\n",
    "  first_img = torch.Tensor.reshape(first_img.to(device), (1,1,148,64))\n",
    "\n",
    "  for i in range(len(dataset_test)):\n",
    "    x = dataset_test.__getitem__(i)\n",
    "    pairs.append(x[1].to(device))\n",
    "\n",
    "    second_img = torch.Tensor.reshape(x[1].to(device), (1,1,148,64))\n",
    "    with torch.set_grad_enabled(False):\n",
    "      outputs = model(first_img, second_img)\n",
    "      preds += outputs\n",
    "      for output in outputs:\n",
    "        if output[0]<output[1]:\n",
    "          top_5[float(output[1])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Sp3jHDNKev4",
    "outputId": "eb6c8747-5f48-4bf5-89af-25ceb1f8c735"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A53880>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABgUlEQVR4nO2YsU7EMAyGf0cVYmC55SQmNiRWNt6ImZfgPXgLViY22HkBJCQ2JJjMUK60Seo6Ncclkr+tafyf4zj2pYDjOI7jOE5tEACAxw9lBADPP/bggbMiD1h4p/Fgzl5HMFnLAvdWgSergI6wbvenHrxYBc51q50XwKVVwBKGfSZSKwI915yhRCBnz7cq0wDYKkJnLSidzbyGbaT5JWzfjB7cqDyoOgYfJ0YBdWtTccTMzJvVAndfAID3jL/Zg5ic52HsokTgITcptpeCOISR06EBOQbc/3wyNKITBRS1ojwTI83Dn4UmBY4nT3IiZZmmUpMxOLhAdBzNHiycRoCi41NWD0AEgOhKmKGpSPj1Iin1ogeviVTaKkSB07Ez9NgvKEJcgqY1tZiJ/yqwVQiQ2DsU2xCym1tAAGwS1dyZ1rtQeSLZBRR9L6hnigKGe4/51haM9n/xHWmxN8nI9UCR4vvORM1fXVtBQgA+TQLS98Td+yUP5P7vOI7jOE5rfAP/c7LZm97FkAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([7.6851e-05, 9.9992e-01], device='cuda:0') tensor(1., dtype=torch.float64)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A537F0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABJElEQVR4nO2YMW4CMRBFZ6JIcIR0kRDiHjldjkJJmz4NN0A5ARegASH9FLC7rHbC2DuAY/RfgzS2H59ZYwtECCGEEJIFAGAaFbR8RwVnllFBw4+IugJn/CUjDAUUVCzYFk9wd4F6golrcMa988RN4PEWTVD+KVBQQLBWdS/DPr279NMoeoK5OfeimHE7q1l1e9AuOzrjXga1imkCEQyn5Qls6YnqdmLH4vw6ugdNEyruAQXPsJXbr/NDEgAADnaA9CNt9KHazYVRy/3RpcNaXhMxFxFYzsQEBvXuRAr+kWCvr6H1GkygUrgHGhWEE6QJvpxx/1C9cqIUfwqCcAKEPwLeQ00UKd1EClIFH9G3EJHl3//rrsb4Zptm+e4G6QghhBDS5xeXVcVLGVFiigAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2.5592e-05, 9.9997e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A532E0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABfElEQVR4nO2YPVLDQAyFn5g0rhkyuUJulAPQ0NNwIo5ACR0dt6Cgo/RQIIqsnfXau5GlGf+h18TjrD4/ydr12oDLNbcYzDfq6BdmMEDqiwcpHbTxSsAlXgd4jI5VNYgMaGvggLjyM6UQWVA6uBC0KbQEdQ0awox9QLM7mB/wef7RLqrtqqJ20KxKZcA7B2Xjr6TQD6x+f7rnxwJ6WnMfOGAigKCPlp6CAxYAuLUC7iSA0poo6eSFF3GDACIiqkYAjkk4AKBObnzx2Rg3Ag2fLqdAg4fdiwpr0DX6lP2nLwaAOq2ceH9QxJ612veFDQBerdc9NDtQwxeMIDNAuVu3fsGI9G8BU05nZn424EO7pp3PkUrh95lhb1JANOwrZ6A0mU7R8X4fxXdGFeZC4o6GT8tvIzOAj17ScgcZrXUuOMABDlggYGcLJ6MDMqZAsAEIRgAcYAfwFcC31cGDyIKtBmwuIvs3lO0DaiugoqBjfszot74TS7fqLpdrYv0BdEaYmY5n/vsAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([3.0024e-05, 9.9997e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A51930>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAA7klEQVR4nO2W0RHCMAiGiacreB3FddzFYRyhG+kW+KDNJaEXobTGtv/3pg0fxAMqEQAAgLnggzOenAI/3iuQ+wp3bwFbIGgOceW8VSBipgiyuKmCGOsREJG/E3cj6JtXAMHCgt8MU62RMM77EfzXQjmHvPdUnZi0Yig+Gyu4BUvahCszM6cjwQNaFxd5o8xcTGlo14lD6SuZBQgWEjTvxDhMR8P50cHTVfDOxxdZgHWliY1mfzs/qcue4fUOAQQQWASP2kPNQqntk5X8BhBAMIegG/22Dx++C8QsZTHGK5xETuV/pDIvAAAAAMCmeQHjBDFRhWPKIQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([4.4922e-05, 9.9996e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A538E0>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABT0lEQVR4nO2XvU0EMRCF3yCQqIKEJq4GOqAHCoCA+OiDhA4QpCR0QExGAeRDsIa99Xm9zztI5ud9iVc+z5tna+yzASGEEEKIKcYMej0BcAecr83iRS55Bz77i+FgratP6aAAPCoQdiABwBiBl0p80IFxpTxbyYbYGhhiAlejygIzUxhCu9dBRMC7OxgshBx4VACuM7GPwLOZ7e7A1t1oeR/j4LSQ76jFwZjO9vuoNbCsBYA3JnDk3j0/l9I1g5pCkSS4vg4sKgAJSOBHCIT3QuKQTpft24vUtpxIVuijpuBZO/kmBI6/vp6IbCUDIw/7fW0Cafx2p4NYxHqS31zKA9R7oRYffy8sC2yq8SEHt9ywTfntzhUhADzWBZYrsfb2R+/LdljgG27rsUUEgJuoQP/tLIG/IHDN/DGV2dJJpnFn7S7d3d/bw4QQQgjxf/kA/0TTRegLZEwAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([7.6851e-05, 9.9992e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A51930>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAABJElEQVR4nO2YMW4CMRBFZ6JIcIR0kRDiHjldjkJJmz4NN0A5ARegASH9FLC7rHbC2DuAY/RfgzS2H59ZYwtECCGEEJIFAGAaFbR8RwVnllFBw4+IugJn/CUjDAUUVCzYFk9wd4F6golrcMa988RN4PEWTVD+KVBQQLBWdS/DPr279NMoeoK5OfeimHE7q1l1e9AuOzrjXga1imkCEQyn5Qls6YnqdmLH4vw6ugdNEyruAQXPsJXbr/NDEgAADnaA9CNt9KHazYVRy/3RpcNaXhMxFxFYzsQEBvXuRAr+kWCvr6H1GkygUrgHGhWEE6QJvpxx/1C9cqIUfwqCcAKEPwLeQ00UKd1EClIFH9G3EJHl3//rrsb4Zptm+e4G6QghhBDS5xeXVcVLGVFiigAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([8.4154e-05, 9.9992e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A53D90>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAA0UlEQVR4nO2WMQ7CMAxFf5iYOQW34C5chisxs7AywXHMAElBZCAxlRvx3lIpal8T2/2qBAAAAPBOarrbPpdW3h0gQIAgQrCtrDXlQSUORqvB+IJaEwY7QrjgUF1t+BaqTRirBhPp5eANgn1+OklpU2Q9O5CmknbXIL950C4sRHD80Q665yAPQlwNvJNYwqFFcKtHytdczcwejosV/jXSECBAMIOgi93zesoJ5cAtAMniB8kpODt+MCRJFn2EtecfSZIsvgtegbOHEnkAAAAAi+QOFAYnmvybedAAAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.0120e-04, 9.9990e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A51E70>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAA4UlEQVR4nO2XwRGCQAxFfxyLkZNnOrEUqvBqKdqBJUAH2kW8ICjEQ5KRHfG/C0yG/GQJ2Q0AIYQQQsiqaBWQkKcOd0efX6MzXBmoYdu4UqCAhWQFdtkypjMo/xJvrqfP81bQ4kvw7QfsRgpQ4DPbiJNgbCzf6awvLr2CbwlSiUxCxuaDcAZG6PJl/GuBdBmzGTT9NfwlPpvJl4Gepv6Bs1GyAgBqXAebR8A6nJct490ycsCgAAUCAhfDFpgP3glNKECFLuS3j4Ujq6f4r+8hMeoCAPTXNpRvYA4dhBBCCCEpHpCqRVGFeHOiAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.1616e-04, 9.9988e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A53640>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAA+ElEQVR4nO2WsRHCMAxFpUDBKnTMwQZMwAIczEBNwSiUjAEtk4giLozPylmYsyPuvyYXxfnWl5WLiAAAAIAfwsb1JyI602Gx29gFJBsdjBn0FMg78GRBceDIguagWQYP9Unht6A6cHQKEKgWWOmPyhpJ76PCDCa28VLEGQjoVexgYc3MzNa/OpGMbNOATeEeBy5GAaJbLim7l1iBHDUSBBSkdQb7pG3H2/JOTNeLMYOwXmQI1xAvzqB2zOs+ZPUX6D/mPWszwDH+h4B63H4szEBAq6IjC/UCShE8WcjzMqw9SsRXu4V3rx/BpVGkarIFAAAAAAAAtOQNDuFNtK+7z70AAAAASUVORK5CYII=\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.2418e-04, 9.9988e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A51E70>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAAvUlEQVR4nO2WMQ7CMAxFf3uKchZWBs7ACTgUO0eDkU6Mn4GKqjSLYwm19L3NlvJj/dhJJAAAAICN43mqSa5XW1kLAtsV6ErJSCeWGnFlHiCAgKRjdi/5w2WMagTO7/Bp245PY+tJHDGxeei6b76KDr1MpZJW1Qdzdkp6ICcruGUr+INTyAqEZ0G9fUhsqG56A7hynDWc/RBVeWCPajzvPxY4lZJ894MC90IudicWXFyZB8sUAAAAAAAAgCXzArjsRufqB+M2AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.5449e-04, 9.9985e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A53640>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAA80lEQVR4nO2WOw7CMBBEZxFCgpoCbsJNOFKOxiG4Ai1KQTEUVqIkygp/kIzFvG7j+Gn8kw0IIYQQQoia8F4qIMnn8qslCFY7bTL6hywdAKCLT8D1z/EJHBoSOFPQ0hCqC7w5bGgI1QUntyXyOLuL0M4cSPAFwWy7vHISPAaTme3MEi60EZLXRU3S36IfuQVBTpYhAoCiVbBSAST4AUHpPgiPrMRnns3LpAQkAPKy6B+dYHrq9kA/Ftu4/sdp0U8L3UwS/Jfg7LbknMacBD4SxAvc1WpnCOUCbxLqDyHyZloj+5V2sMBUEwXTfhdCCCGEEBV5AykvN1z8KF9aAAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.7059e-04, 9.9983e-01], device='cuda:0')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x148 at 0x7FF8F1A51E70>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAACUCAAAAAAGrXA7AAAA5UlEQVR4nO2WPQ7CMAyFbe7DDCMjR4GBIzDDDXqMnoQRcRQGBrNQSqO6eiFVolTv2/L3/Go7UUUIIYQQQsjsKLpxtxERvcj5dY0/LCJio+zh8+24gBnqwryFFexhsQKay8HDM4A2k1dFTcyBFq+CZXPg3oRkB5b8CVb8LmQS2E6sQXdhooq15GBWgVMwxh6UbxZ1OIx0cNNPuHW0A9fRv0n8CVuqjL2FChuJAh19K2ICT3+MP2kaTuAC3XYNJ0CBMNzwiY0TGKHaPqAABShAAQpQYJEC9+MB+kFpmpQohBBCCCH18QaW5EuFrZ1DYwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "pil_image = torchvision.transforms.ToPILImage()(img_compared)\n",
    "pil_image.show()\n",
    "\n",
    "print(preds[id], first_label)\n",
    "pil_image = torchvision.transforms.ToPILImage()(pairs[id])\n",
    "pil_image.show()\n",
    "\n",
    "sorted_keys = sorted(top_5.keys(),reverse=True)\n",
    "for i in sorted_keys[:10]:\n",
    "  print(preds[top_5[i]])\n",
    "  pil_image = torchvision.transforms.ToPILImage()(pairs[top_5[i]])\n",
    "  pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qnHk9dvJgFa"
   },
   "source": [
    "### TOP_10 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s74Ty_kOJe9y"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "pairs = []\n",
    "list_top_5 = []\n",
    "with torch.no_grad():\n",
    "  for id in range(len(dataset_test)):\n",
    "    top_5 = {}\n",
    "    img_compared = dataset_test.__getitem__(id)[0]\n",
    "    img_compared = torch.Tensor.reshape(img_compared.to(device), (1,1,148,64))\n",
    "    perfect_pair = dataset_test.__getitem__(id)[1].to(device)\n",
    "    is_match = int(dataset_test.__getitem__(id)[2])\n",
    "\n",
    "    if is_match:\n",
    "      for i in range(len(dataset_test)):\n",
    "        x = dataset_test.__getitem__(i)\n",
    "        # pairs.append(x[1].to(device))\n",
    "\n",
    "        second_img = torch.Tensor.reshape(x[1].to(device), (1,1,148,64))\n",
    "        with torch.set_grad_enabled(False):\n",
    "          outputs = model(img_compared, second_img)\n",
    "          preds += outputs\n",
    "          for output in outputs:\n",
    "            if output[0]<output[1]:\n",
    "              top_5[float(output[1])] = i\n",
    "        \n",
    "      list_top_5.append(top_5)\n",
    "    else:\n",
    "      list_top_5.append({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1G_iRIgK2JQ",
    "outputId": "6e2a9d5c-7a87-4f0b-aeca-2cf06443a516",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(list_top_5[0])\n",
    "test_df = pd.read_csv('/content/gdrive/MyDrive/puzzle_pieces/puzzle_pairs_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skeMqWAG1bL7",
    "outputId": "1ffc3976-c412-46d6-d4b6-7dbca56c32e7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "162 20\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "correct=0\n",
    "for id in range(len(dataset_test)): \n",
    "  label = dataset_test.__getitem__(id)[2]\n",
    "  # print(test_df.loc[id][0], test_df.loc[id][1], int(label), preds[id])\n",
    "  if int(label) == 1:\n",
    "    total += 1\n",
    "    top_5_id = list_top_5[id]\n",
    "    sorted_keys = sorted(top_5_id.keys(),reverse=True)\n",
    "    for i in sorted_keys[:10]:\n",
    "      # print(test_df.loc[top_5_id[i]][1], i)\n",
    "      if test_df.loc[top_5_id[i]][1] == test_df.loc[id][1]:\n",
    "        correct += 1\n",
    "  # print(\"-------------------\")\n",
    "print(total, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test on new images "
   ],
   "metadata": {
    "id": "fXzuAKN4SKDT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_path = \"/content/gdrive/MyDrive/models/inverted_img_no_dropout.pt\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.load(model_path, device)"
   ],
   "metadata": {
    "id": "Mm8Nsq0mSQis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "preds=[]\n",
    "with torch.no_grad():\n",
    "  for id in range(1):\n",
    "    img1 = Image.open(\"/content/v3.png\").convert('L')\n",
    "    w,h = 148,148\n",
    "    img1 = transforms.functional.crop(img1,0,int(w/2)+10,h,int(w/2)-10) # top, left, height, width\n",
    "    img1.show()\n",
    "    img1 = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])(img1)\n",
    "    img1_tensor = torch.Tensor.reshape(img1.to(device), (1,1,148,64))\n",
    "\n",
    "    img2 = PIL.ImageOps.invert(Image.open(\"/content/v2.png\").convert('L'))\n",
    "    img2 = transforms.functional.crop(img2,0,0,h,int(w/2)-10)\n",
    "    img2 = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])(img2)\n",
    "    img2_tensor = torch.Tensor.reshape(img2.to(device), (1,1,148,64))\n",
    "    \n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "      outputs = model(img1_tensor, img2_tensor)\n",
    "      preds += outputs"
   ],
   "metadata": {
    "id": "Fy4gpDUoFVj3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "outputId": "66c12395-dd7d-4998-db66-17b2e0389720",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(preds)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXrbOhSdSMTM",
    "outputId": "cea14ddc-aabf-4a21-fdc7-40a15fa27492"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor([0.4710, 0.5290], device='cuda:0')]\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "TSe6hfLsMIh1",
    "mhrP5H3HMqd3",
    "Dr2W_Ehcdk7p",
    "Zy6r_Q8lazrD",
    "SZn57Q2nluDc",
    "17qnP-2vES9z",
    "0qnHk9dvJgFa",
    "fXzuAKN4SKDT"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
